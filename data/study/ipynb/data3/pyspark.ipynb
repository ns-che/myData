{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b145bcb-5c3f-4c8a-99fe-bae213193038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- -----------\n",
      "anyio                     4.10.0\n",
      "argon2-cffi               21.3.0\n",
      "argon2-cffi-bindings      25.1.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.4.0\n",
      "babel                     2.16.0\n",
      "backcall                  0.2.0\n",
      "beautifulsoup4            4.13.5\n",
      "bleach                    6.2.0\n",
      "Bottleneck                1.4.2\n",
      "brotlicffi                1.0.9.2\n",
      "certifi                   2025.10.5\n",
      "cffi                      2.0.0\n",
      "charset-normalizer        3.4.4\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.1\n",
      "debugpy                   1.8.16\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "dnspython                 2.7.0\n",
      "exceptiongroup            1.3.0\n",
      "executing                 2.2.1\n",
      "fastjsonschema            2.20.0\n",
      "findspark                 2.0.1\n",
      "h11                       0.16.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "idna                      3.11\n",
      "importlib_metadata        8.5.0\n",
      "ipykernel                 6.30.1\n",
      "ipython                   8.15.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "ipywidgets                8.1.7\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "joblib                    1.5.2\n",
      "json5                     0.9.25\n",
      "jsonschema                4.25.0\n",
      "jsonschema-specifications 2023.7.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.8.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.7\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.28.0\n",
      "jupyterlab_widgets        3.0.15\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.2\n",
      "mkl_fft                   1.3.11\n",
      "mkl_random                1.2.8\n",
      "mkl-service               2.4.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook                  7.4.5\n",
      "notebook_shim             0.2.4\n",
      "numexpr                   2.10.1\n",
      "numpy                     2.0.1\n",
      "overrides                 7.4.0\n",
      "packaging                 25.0\n",
      "pandas                    2.3.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pickleshare               0.7.5\n",
      "pip                       25.2\n",
      "platformdirs              4.3.7\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.52\n",
      "psutil                    7.0.0\n",
      "pure_eval                 0.2.3\n",
      "py4j                      0.10.9.7\n",
      "pyarrow                   21.0.0\n",
      "pycparser                 2.23\n",
      "Pygments                  2.19.1\n",
      "pymongo                   4.15.4\n",
      "PySocks                   1.7.1\n",
      "pyspark                   3.5.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytz                      2025.2\n",
      "pywin32                   311\n",
      "pywinpty                  2.0.15\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     27.1.0\n",
      "qtconsole                 5.7.0\n",
      "QtPy                      2.4.3\n",
      "referencing               0.30.2\n",
      "requests                  2.32.5\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.22.3\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.13.1\n",
      "Send2Trash                1.8.2\n",
      "setuptools                80.9.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.0\n",
      "soupsieve                 2.5\n",
      "stack_data                0.6.3\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.4.0\n",
      "tomli                     2.2.1\n",
      "tornado                   6.5.1\n",
      "traitlets                 5.14.3\n",
      "typing_extensions         4.15.0\n",
      "tzdata                    2025.2\n",
      "urllib3                   2.5.0\n",
      "wcwidth                   0.2.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.14\n",
      "win_inet_pton             1.1.0\n",
      "zipp                      3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155b9450-d358-4a1d-b3c1-c071d033cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPRK_PYTHON'] = \"python\"\n",
    "os.environ['PYSPRK_DRIVER_PYTHON_'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27558d93-73a0-4bf9-ab3a-a615abf794c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7bff98-c589-4534-9c66-39cac5f079ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| eee| 45| 65|  78|     98|  286|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| iii|100| 78|  56|     65|  299|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('PySpark Example')\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data/Subjects.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c13551-a6cf-4b57-bdce-166962377685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| eee| 45| 65|  78|     98|  286|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| iii|100| 78|  56|     65|  299|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n",
      "+-----------------+\n",
      "|       avg(total)|\n",
      "+-----------------+\n",
      "|312.3333333333333|\n",
      "+-----------------+\n",
      "\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼 추가\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df.show()\n",
    "\n",
    "# 평균 계산\n",
    "df.groupBy().avg(\"total\").show()\n",
    "\n",
    "# 조건 필터링: 총점 300 이상 합격\n",
    "df.filter(col(\"total\")>=300).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2939f81e-dff2-4d34-9e4b-23ce6921383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|total|\n",
      "+----+-----+\n",
      "| aaa|  342|\n",
      "| ccc|  338|\n",
      "| ddd|  346|\n",
      "| fff|  341|\n",
      "| hhh|  343|\n",
      "| jjj|  362|\n",
      "| lll|  319|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temp View 생성\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "\n",
    "# SQL 쿼리\n",
    "high_score = spark.sql(\"SELECT name, total FROM students WHERE total >= 300\")\n",
    "high_score.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d6dc16-e0e2-48a3-aa2f-8f65f463aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksession 종료\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f183c12-aef2-4002-b670-cd82077ef57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "239b0d5e-70b4-4b57-8758-46d56ab9d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksession : 메인 진입접 master(\"local[*]\") : cpu 코어 전체 사용\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"PySpark_ML_Example\")\\\n",
    "    .master(\"Local[*]\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a9c448e-e09d-4fd4-a5ea-c8c1fe729544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+\n",
      "|class|name|kor|eng|math|science|\n",
      "+-----+----+---+---+----+-------+\n",
      "|    1| aaa| 67| 87|  90|     98|\n",
      "|    1| bbb| 45| 45|  56|     98|\n",
      "|    1| ccc| 95| 59|  96|     88|\n",
      "|    1| ddd| 65| 94|  89|     98|\n",
      "|    1| eee| 45| 65|  78|     98|\n",
      "|    1| fff| 78| 76|  98|     89|\n",
      "|    2| ggg| 87| 67|  65|     56|\n",
      "|    2| hhh| 89| 98|  78|     78|\n",
      "|    2| iii|100| 78|  56|     65|\n",
      "|    2| jjj| 99| 89|  87|     87|\n",
      "|    2| kkk| 98| 45|  56|     54|\n",
      "|    2| lll| 65| 89|  87|     78|\n",
      "+-----+----+---+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# csv 파일로드(첫번째 행을 컬럼명, 데이터 타입 자동 추론)\n",
    "df = spark.read.csv('data/Subjects.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2b9f36-4e7b-4031-8a80-e55057f82352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+----+\n",
      "|class|name|kor|eng|math|science|total|pass|\n",
      "+-----+----+---+---+----+-------+-----+----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|   1|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|   0|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|   1|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|   1|\n",
      "|    1| eee| 45| 65|  78|     98|  286|   0|\n",
      "|    1| fff| 78| 76|  98|     89|  341|   1|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|   0|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|   1|\n",
      "|    2| iii|100| 78|  56|     65|  299|   0|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|   1|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|   0|\n",
      "|    2| lll| 65| 89|  87|     78|  319|   1|\n",
      "+-----+----+---+---+----+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결측값은 0으로(데이터 전처리), 총점및 합격여부 컬럼(파생변수)\n",
    "df = df.fillna({\"kor\":0, \"eng\":0, \"math\":0, \"science\":0})\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df = df.withColumn(\"pass\", when(col(\"total\") >= 300, 1).otherwise(0))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d79d56-3871-4972-b104-59a3b53a0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 모델/ 입력 피쳐 - 전체 / 타겟 - total\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"kor\", \"eng\", \"math\", \"science\"],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61f98b1b-1087-44be-8ba3-5bcf9b60f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+----+--------------------+\n",
      "|class|name|kor|eng|math|science|total|pass|            features|\n",
      "+-----+----+---+---+----+-------+-----+----+--------------------+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|   1|[67.0,87.0,90.0,9...|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|   0|[45.0,45.0,56.0,9...|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|   1|[95.0,59.0,96.0,8...|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|   1|[65.0,94.0,89.0,9...|\n",
      "|    1| eee| 45| 65|  78|     98|  286|   0|[45.0,65.0,78.0,9...|\n",
      "|    1| fff| 78| 76|  98|     89|  341|   1|[78.0,76.0,98.0,8...|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|   0|[87.0,67.0,65.0,5...|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|   1|[89.0,98.0,78.0,7...|\n",
      "|    2| iii|100| 78|  56|     65|  299|   0|[100.0,78.0,56.0,...|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|   1|[99.0,89.0,87.0,8...|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|   0|[98.0,45.0,56.0,5...|\n",
      "|    2| lll| 65| 89|  87|     78|  319|   1|[65.0,89.0,87.0,7...|\n",
      "+-----+----+---+---+----+-------+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = assembler.transform(df)\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38d393b2-8e23-4694-a45f-be3d8438034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|total|\n",
      "+--------------------+-----+\n",
      "|[67.0,87.0,90.0,9...|  342|\n",
      "|[45.0,45.0,56.0,9...|  244|\n",
      "|[95.0,59.0,96.0,8...|  338|\n",
      "|[65.0,94.0,89.0,9...|  346|\n",
      "|[45.0,65.0,78.0,9...|  286|\n",
      "|[78.0,76.0,98.0,8...|  341|\n",
      "|[87.0,67.0,65.0,5...|  275|\n",
      "|[89.0,98.0,78.0,7...|  343|\n",
      "|[100.0,78.0,56.0,...|  299|\n",
      "|[99.0,89.0,87.0,8...|  362|\n",
      "|[98.0,45.0,56.0,5...|  253|\n",
      "|[65.0,89.0,87.0,7...|  319|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = assembler.transform(df).select(\"features\", \"total\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90c085c4-f2b1-4d72-bf1c-ed5f3987af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|total|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[67.0,87.0,90.0,9...|  342| 342.0000000000004|\n",
      "|[45.0,45.0,56.0,9...|  244|243.99999999999918|\n",
      "|[95.0,59.0,96.0,8...|  338|338.00000000000153|\n",
      "|[65.0,94.0,89.0,9...|  346| 346.0000000000002|\n",
      "|[45.0,65.0,78.0,9...|  286|285.99999999999903|\n",
      "|[78.0,76.0,98.0,8...|  341|341.00000000000045|\n",
      "|[87.0,67.0,65.0,5...|  275|274.99999999999847|\n",
      "|[89.0,98.0,78.0,7...|  343| 343.0000000000002|\n",
      "|[100.0,78.0,56.0,...|  299|298.99999999999994|\n",
      "|[99.0,89.0,87.0,8...|  362|362.00000000000153|\n",
      "|[98.0,45.0,56.0,5...|  253|252.99999999999912|\n",
      "|[65.0,89.0,87.0,7...|  319| 318.9999999999987|\n",
      "+--------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# featureCol:독립변수 벡터 labelCol: 종속변수 .fit: 회귀계수(weight)와 절편(intercept) 학습\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# 예측 수행\n",
    "lr_predictions = lr_model.transform(train_df)\n",
    "# lr_predictions.select(\"features\", \"total\", \"prediction\").show(5)\n",
    "lr_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "011228ba-799a-4f2d-9bdc-2f5759bde31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델(logistic regression)/입력:과목피쳐/타겟:합격여부/목적:합불합 분류\n",
    "\n",
    "assembler2 = VectorAssembler(\n",
    "    inputCols=[\"kor\", \"eng\", \"math\", \"science\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_df2 = assembler2.transform(df).select(\"features\", \"pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f3ce9dd-38f8-487c-9a47-e004988884b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+----------+\n",
      "|            features|pass|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----+--------------------+--------------------+----------+\n",
      "|[67.0,87.0,90.0,9...|   1|[-19.686724629161...|[2.81944823643108...|       1.0|\n",
      "|[45.0,45.0,56.0,9...|   0|[59.4855269309313...|           [1.0,0.0]|       0.0|\n",
      "|[95.0,59.0,96.0,8...|   1|[-18.201306704974...|[1.24529696368858...|       1.0|\n",
      "|[65.0,94.0,89.0,9...|   1|[-22.409196735736...|[1.85271882249817...|       1.0|\n",
      "|[45.0,65.0,78.0,9...|   0|[17.5872602642891...|[0.99999997698823...|       0.0|\n",
      "|[78.0,76.0,98.0,8...|   1|[-26.642164741617...|[2.68815846669923...|       1.0|\n",
      "|[87.0,67.0,65.0,5...|   0|[17.9887252920324...|[0.99999998459733...|       0.0|\n",
      "|[89.0,98.0,78.0,7...|   1|[-19.278335486291...|[4.24155669289329...|       1.0|\n",
      "|[100.0,78.0,56.0,...|   0|[18.5685481242207...|[0.99999999137454...|       0.0|\n",
      "|[99.0,89.0,87.0,8...|   1|[-27.805775496769...|[8.39663232819672...|       1.0|\n",
      "|[98.0,45.0,56.0,5...|   0|[40.7304563598262...|           [1.0,0.0]|       0.0|\n",
      "|[65.0,89.0,87.0,7...|   1|[-17.302778378604...|[3.05843190344907...|       1.0|\n",
      "+--------------------+----+--------------------+--------------------+----------+\n",
      "\n",
      "+----------------------+----+----------+------------------------------------------+\n",
      "|features              |pass|prediction|probability                               |\n",
      "+----------------------+----+----------+------------------------------------------+\n",
      "|[67.0,87.0,90.0,98.0] |1   |1.0       |[2.81944823643108E-9,0.9999999971805518]  |\n",
      "|[45.0,45.0,56.0,98.0] |0   |0.0       |[1.0,0.0]                                 |\n",
      "|[95.0,59.0,96.0,88.0] |1   |1.0       |[1.245296963688586E-8,0.9999999875470303] |\n",
      "|[65.0,94.0,89.0,98.0] |1   |1.0       |[1.852718822498173E-10,0.9999999998147281]|\n",
      "|[45.0,65.0,78.0,98.0] |0   |0.0       |[0.9999999769882363,2.3011763716773714E-8]|\n",
      "|[78.0,76.0,98.0,89.0] |1   |1.0       |[2.688158466699238E-12,0.9999999999973118]|\n",
      "|[87.0,67.0,65.0,56.0] |0   |0.0       |[0.9999999845973353,1.540266469923779E-8] |\n",
      "|[89.0,98.0,78.0,78.0] |1   |1.0       |[4.241556692893291E-9,0.9999999957584433] |\n",
      "|[100.0,78.0,56.0,65.0]|0   |0.0       |[0.9999999913745451,8.625454905875074E-9] |\n",
      "|[99.0,89.0,87.0,87.0] |1   |1.0       |[8.396632328196729E-13,0.9999999999991603]|\n",
      "|[98.0,45.0,56.0,54.0] |0   |0.0       |[1.0,0.0]                                 |\n",
      "|[65.0,89.0,87.0,78.0] |1   |1.0       |[3.0584319034490715E-8,0.9999999694156809]|\n",
      "+----------------------+----+----------+------------------------------------------+\n",
      "\n",
      "Column<'pass'>\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(featuresCol = \"features\", labelCol = \"pass\")\n",
    "logr_model = logr.fit(train_df2)\n",
    "\n",
    "logr_predictions = logr_model.transform(train_df2)\n",
    "logr_predictions.show()\n",
    "logr_predictions.select(\"features\", \"pass\", \"prediction\", \"probability\").show(truncate=False)\n",
    "print(col(\"pass\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19c04b9b-379b-488d-8ae8-75c84514be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad145277-56f7-4ff1-ae67-fd7c4bbacb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MongoDB_Spark_Test\")\\\n",
    "#     .master(\"local[*]\")\\\n",
    "#     .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\")\\\n",
    "#     .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/testdb.students\")\\\n",
    "#     .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/testdb.students_result\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# my_spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"myApp\") \\\n",
    "#     .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "#     .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1:27017/testdb.coll\") \\\n",
    "#     .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1:27017/testdb.coll\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoSparkExample\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark/bin/mongo-spark-connector_2.13-10.5.0.jar\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1/testdb.students\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1/testdb.students\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79665b87-f108-445d-8fb0-5612ef326c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o149.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# df = spark.read\\\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     .format(\"mongo\")\\\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     # .option(\"uri\", \"mongodb://localhost:27017/testdb.students\")\\\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     .load()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMongoDB 데이터 로드 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o149.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read\\\n",
    "#     .format(\"mongo\")\\\n",
    "#     # .option(\"uri\", \"mongodb://localhost:27017/testdb.students\")\\\n",
    "#     .load()\n",
    "\n",
    "df = spark.read.format(\"mongodb\").load()\n",
    "\n",
    "print(\"MongoDB 데이터 로드 완료\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b65f7872-6688-4b84-9007-e8b465cf06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df = df.withColumn(\"pass\", when(col(\"total\") >= 300, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60729e24-0811-4e33-9342-c95f60ce11b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1978.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai4\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1978.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "df.write\\\n",
    "    .format(\"mongodb\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4c64d-e5fd-4c97-8094-d9d1b0fb68f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
